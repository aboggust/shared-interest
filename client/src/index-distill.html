<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-149275901-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-149275901-2');
    </script> -->
    <meta charset="UTF-8">
    <meta name="title" content="SaliencyIOU" />
    <meta name="description" content="DESCRIPTION" />

    <script src="https://distill.pub/template.v2.js"></script>

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
        integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans:300,400&display=swap" rel="stylesheet">
    <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/css/bootstrap-select.min.css">

    <title>Saliency Explorer</title>
    <link href="main.css" rel="stylesheet">
</head>

<body>

    <d-front-matter>
        <script src="assets/frontmatter.json" type="application/json"></script>
    </d-front-matter>

    <d-title>
        <h1>Saliency Explorer</h1>
        <p>Explore model reasoning across images using ground truth annotations. </p>
    </d-title>

    <div id="full-app"></div>

    <dt-article>

        <h1>Abstract</h1>

        <p>As deep learning is applied to high stakes scenarios, it is increasingly important that a model is not only
            making accurate decisions, but doing so for the right reasons. Common explainability methods provide pixel
            attributions as an explanation for a model's decision on a single image. However, using these input-level
            explanations to understand patterns in model behavior is challenging for large datasets as it requires
            selecting and analyzing an interesting subset of inputs. By utilizing the human-generated bounding boxes
            that represent ground-truth object locations, we introduce metrics for scoring and ranking inputs based on
            the correspondence between the input’s ground-truth object location and the explainability method’s
            explanation region. Our methodology is agnostic to model architecture, explanation method, and image dataset
            allowing it to be applied to many tasks and domains.
            We apply our method to two high profile scenarios: an open source ImageNet model widely used in the
            community and a melanoma prediction model, showing it surfaces patterns in model behavior by identifying
            inputs that are "right for the right reasons" <d-footnote>Or, even, "wrong for the right reasons"
            </d-footnote>.
        </p>

        <h1>Introduction</h1>

        <p>As deep learning is applied to high-stakes applications such as cancer diagnosis[], autonomous driving[], and
            facial recognition[], it becomes increasingly important to understand the decisions being made by machine
            learning models. Various prior work has demonstrated weaknesses in these models including reliance on
            non-salient regions[] or on background information only[]. Saliency methods, such as LIME[] or integrated
            gradients[], help identify these pitfalls by providing model explanations, allowing humans to identify the
            features on which a model decision is based. However, these methods provide explanations on the image-level
            making it challenging to understand global model behavior or dataset limitations. </p>


        <p>
            In this work, we use saliency methods in conjunction with the ground-truth object bounding boxes provided in
            many computer vision datasets to explore model decisions. We introduce three scoring functions --- saliency
            coverage, bounding box coverage, and IOU --- to sort images based on the overlap between the explanation and
            the
            ground-truth object location. By sorting images in this way, we discover insights into when and why the
            model
            was “right for the right reasons”, “wrong for the wrong reasons”, or perhaps most interestingly “right for
            the
            wrong reasons”. We show our methodology is applicable to various model architectures, explanatory methods,
            and
            input datasets by evaluating on two representative tasks: ImageNet vehicle classification[] using a
            ResNet50[]
            and melanoma prediction[] using VGG11[]. In both tasks, we identify insights into model behavior, finding
            failures in dataset labels and model bias.
        </p>

        <h1>Related Work</h1>

        <h3>Image Datasets</h3>

        <p>Image datasets from across domains increasingly include ground-truth bounding box annotations. Annotations
            have been applied for downstream tasks such as object classification and object localization [Imagenet,
            CoCo, Stanford Dogs Dataset, PASCAL], facial recognition [CelebFaces, Face Dataset with Emotion, Age,
            Ethnicity], and biological classification [ISIC Lesions, NIH Chest XRays, NIH Deep Lesion, Malaria Cells
            (Broad)]. In our work, we utilize ground truth annotations to improve model explainability.
        </p>

        <h3>Saliency Methods</h3>

        <p>Instance-level saliency methods, such as vanilla gradients[], integrated gradients[], LIME[], GradCAM[],
            SmoothGrad[], and SIS[], provide pixel level explanations for model behavior. However, these methods operate
            on single model inputs making it challenging to find patterns of model behavior over large datasets. Our
            method utilizes the explanations from these methods to rank inputs, allowing users to understand model
            behavior in aggregate.
        </p>

        <h1>Method</h1>

        <p>In our method, we leverage the ground truth annotations along with instance-level explanations to compute
            coverage scores for each image. Sorting the images using these scores, we are able to query for instances
            that give us insight into model behavior. Our methodology is agnostic to model architecture, dataset, and
            saliency technique, giving it the flexibility to be used across tasks and domains.
        </p>

        <p>
            We compute three coverage metrics to allow a breadth of exploration: explanation coverage (Eq. 1), ground
            truth coverage (Eq. 2), and IOU (Eq. 3). To compute each metric, we assume the input is a set of pixels (GT)
            corresponding to the region within the known ground truth region and a set of pixels (E) corresponding to
            the pixels within the explanation region.
        </p>


        <d-math block="">
            \text{Explanation Coverage} = \frac{|GT \cap E|}{|E|}
        </d-math>

        <d-math block="">
            \text{Ground Truth Coverage} = \frac{|GT \cap E|}{|GT|}
        </d-math>

        <d-math block="">
            \text{IOU} = \frac{|GT \cap E|}{|GT \cup E|}
        </d-math>

        <p> </p>

        <style></style>

        <d-figure>
            <figure style="margin-top: 10px">
                <img src="./assets/img/explain_01.png" alt="Explaining ground truth pt1"
                    style="margin-top: 20px; margin-bottom: 20px;">
                <img src="./assets/img/explain_02a.png" alt="Explaining ground truth pt2" style="margin-bottom: 20px;">
                <figcaption>Explanation coverage represents the proportion of the explanation region covered by the
                    ground truth region. High explanation coverage indicates the entire explanation region lies within
                    the ground truth region, meaning the model is relying on a subset of salient features to make its
                    prediction. Filtering for correctly classified inputs with high explanation coverage can surface
                    instances where a subset of the object, such as the dog’s face, was sufficient to make a correct
                    prediction. Looking at incorrectly classified images with high explanation coverage can help us find
                    instances where the model uses an insufficient portion of the object to make a prediction (e.g.,
                    using a small region of black and white spots to predict dalmation).
                </figcaption>
            </figure>
        </d-figure>

        <p>Ground truth coverage represents the proportion of the ground truth region covered by the explanation region.
            High ground truth coverage indicates that the model is using the entire object to make its decision. In
            Table 1, we show examples where filtering correctly classified images with high ground truth coverage
            uncovers instances where the model relies on the object and relevant background pixels (e.g., the cab and
            the street), to make a correct prediction. Incorrectly classified instances with high ground truck coverage
            show examples where the model over relies on contextual information such as using the keyboard and person’s
            lap to predict laptop.
        </p>

        <p>IOU is the strictest metric. A high IOU score indicates the explanation and ground truth are very similar and
            IOU = 1 when the explanation and ground truth regions are identical. Looking at correctly classified images
            with high IOU scores can find instances where the model was right for the exactly the right reasons.
            Incorrectly classified images with high IOU scores can surface examples where the image labels are ambiguous
            such as moped and motor scooter.

        </p>

        <h1>Pretrained Vehicle Classification</h1>

        <p>We evaluate our methodology on the PyTorch torchvision[] ResNet50[] pretrained on ImageNet[]. Since this
            model is commonly used by the PyTorch community for out of the box classification, finetuning, and transfer
            learning, it is crucial that its decisions are grounded in salient features. In this case study, we evaluate
            the model on an ImageNet classification task on a subset of ImageNet9[] images containing vehicles. Each
            image in the subset contains a single ground truth bounding box annotation for its label object. For each
            image we compute a LIME explanation[]. Using the overlap metrics between the explanation and the ground
            truth bounding box, our methodology allows us to gain insight into model behavior and the underlying
            dataset.
        </p>


        <p>In Figure 1a, we see Jeep images that have been correctly classified and have high IOU scores.
            In each of the images, the model explanation has high agreement with the human annotated bounding box.
        </p>

        <p>
            However, looking at the correctly classified Jeep images with low IOU scores in Figure 1b, we see an example
            where the model uses pixels of a black dog to explain its prediction of Jeep. Since the dog can be used to
            make a prediction of Jeep, it may contain adversarial properties that could cause the prediction of Jeep for
            any image edited to contain the dog.
        </p>

        <p>
            Finally, in Figure 1c we look at incorrectly classified images with low saliency coverage. We find that
            despite the images only having a single object label and bounding box, they often contain many objects and
            the labeled object may be in the background.
        </p>

        <p>
            Our method gives insight into the pretrained PyTorch model predictions on vehicle images from ImageNet,
            showing the model uses human interpretable explanations for some classes, the model makes a correct
            prediction using semantically irrelevant regions, and the dataset contains challenging images with multiple
            objects.
        </p>

        <d-figure id="fig1">
            <figure>
                <div id="fig1-0"></div>
                <figcaption>Figure 1a</figcaption>
            </figure>
            <figure>
                <div id="fig1-1"></div>
                <figcaption>Figure 1b</figcaption>
            </figure>
            <figure>
                <div id="fig1-2"></div>
                <figcaption>Figure 1c</figcaption>
            </figure>
        </d-figure>

        <h1>Melanoma Diagnosis</h1>

        <p>
            In our second case study, we evaluate our method using a melanoma diagnostic task. This case study
            represents a real-world scenario where AI-based melanoma classification models[] exist online for at-home
            risk assessments. Since this is a high-stakes task with impact to patient health, it is critical the models
            rely on salient features when making a prediction.
        </p>

        <p>
            In this case study, we use dermoscopic image data from the ISIC Skin Lesion Analysis Towards Melanoma
            Detection 2016 Challenge. Each image in the dataset is an upclose image of a skin lesion labeled as either
            benign or malignant and is annotated with a lesion segmentation created by an expert clinician. We train a
            VGG11 pretrained on ImageNet to learn a binary benign/malignant classification from the original images and
            achieve accuracy on par with 2016 challenge winners. From the trained model, we extract LIME explanations[].
            We evaluate the model by applying our metrics to the ground-truth lesion segmentations and LIME
            explanations.
        </p>

        <p>
            We begin our exploration by analyzing correctly classified images with the highest IOU scores. These
            examples show the instances where the lesion segmentation and explanation region are most similar. We see
            there are a number of images for which the explanation is focused on the lesion, suggesting the model has
            learned a relationship between lesion characteristics and malignancy.
        </p>

        <p>
            Since our model seems to be learning some salient features, we next filter to malignant lesions incorrectly
            classified as benign. Sorting by low ground truth coverage, we see there are instances where our model makes
            incorrect predictions relying only on peripheral skin regions. This is particularly concerning in the case
            of at home risk assessment where a cancerous lesion could be classified as benign due to skin surrounding
            the lesion.
        </p>

        <p>
            Since our model incorrectly classified malignant lesions using non-salient background information, we
            explore if the model can also correctly classify lesions without looking at the lesion. We filter to
            correctly classified benign lesions and look for images with low saliency score. We find a number of images
            where the model relies on the existence of in-frame dermatological tools to make a benign prediction. While
            not salient, these dermatological tools only exist in benign images and are sufficient to make a correct
            classification.
        </p>

        <p>
            Using our methodology reveals insight into melanoma model behavior showing that while the model uses salient
            pixels to make some decisions, it dangerously misclassifies malignant tumors due to peripheral skin regions
            and latches onto spurious dataset features.
        </p>

        <d-figure id="fig2">
            <figure>
                <div id="fig2-0"></div>
                <figcaption>Figure 2a</figcaption>
            </figure>
            <figure>
                <div id="fig2-1"></div>
                <figcaption>Figure 2b</figcaption>
            </figure>
            <figure>
                <div id="fig2-2"></div>
                <figcaption>Figure 2c</figcaption>
            </figure>
        </d-figure>

        <h1>Conclusion</h1>

        <p>
            In this work, we present a methodology that enables humans to understand model behavior using the alignment
            between ground truth object labels and saliency method explanations. Our method is agnostic to model
            architecture, saliency method, and image dataset, allowing it to be used in a range of applications. Using
            real-world case studies, we use our methodology to analyze a pretrained PyTorch model on vehicle
            classification and a melanoma prediction model. These case studies show our method allows users to identify
            where the model is “right for the right reasons”, when the model makes correct predictions using non-salient
            features, and when the dataset contains unexpected features.
        </p>

    </dt-article>

    <script src="https://d3js.org/d3.v4.js"></script>
    <script src="https://d3js.org/d3-scale-chromatic.v1.js"></script>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
        integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
        integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
        crossorigin="anonymous"></script>
    <script type="text/javascript" src="vendor.js"></script>
    <script type="text/javascript" src="distillMain.js"></script>
</body>

</html>