<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-149275901-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-149275901-2');
    </script> -->
    <meta charset="UTF-8">
    <meta name="title" content="SaliencyIOU" />
    <meta name="description" content="DESCRIPTION" />

    <script src="https://distill.pub/template.v2.js"></script>

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
        integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans:300,400&display=swap" rel="stylesheet">
    <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/css/bootstrap-select.min.css">

    <title>Shared Interest: Human Annotations vs. AI Saliency</title>
    <link href="main.css" rel="stylesheet">
</head>

<body>

    <d-front-matter>
        <script id='distill-front-matter' type="text/json">{
        "title": "Shared Interest: Human Annotations vs. AI Saliency",
        "description": "Explore model reasoning across images using ground truth annotations.",
        "authors": [
            {
                "author":"Angie Boggust",
                "authorURL":"http://angieboggust.com/",
                "affiliations": [{"name": "MIT CSAIL", "url": "https://www.csail.mit.edu/"}]
            },
            {
                "author":"Benjamin Hoover",
                "authorURL":"",
                "affiliations": [{"name": "IBM Research", "url": "https://www.draco.res.ibm.com/"}]
            },
            {
                "author":"Arvind Satyanarayan",
                "authorURL":"https://arvindsatya.com/",
                "affiliations": [{"name": "MIT CSAIL", "url": "https://www.csail.mit.edu/"}]
            },
            {
                "author":"Hendrik Strobelt",
                "authorURL":"http://hendrik.strobelt.com/",
                "affiliations": [{"name": "IBM Research", "url": "https://www.draco.res.ibm.com/"}]
            }
        ],
        "katex": {
            "delimiters": [
                {"left": "$$", "right": "$$", "display": false}
            ]
        }
      }</script>
    </d-front-matter>

    <d-title>
        <figure class='l-body' style="margin-top: 2vh; margin-bottom: 2vh;">
            <img src="./assets/img/teaser.svg" style="width:100%;" />
        </figure>
        <p>As deep learning is applied to high stakes scenarios, it is increasingly important that a model is not only
            making accurate decisions, but doing so for the right reasons. Common explainability methods provide pixel
            attributions as an explanation for a model's decision on a single image. However, using these input-level
            explanations to understand patterns in model behavior is challenging for large datasets as it requires
            selecting and analyzing an interesting subset of inputs. By utilizing the human generated bounding boxes
            that represent ground truth object locations, we introduce metrics for scoring and ranking inputs based on
            the correspondence between the input’s ground truth object location and the explainability method’s
            explanation region. Our methodology is agnostic to model architecture, explanation method, and image dataset
            allowing it to be applied to many tasks and domains. We apply our method to two high profile scenarios: an
            open source image classification model widely used in the community and a melanoma prediction model, showing it surfaces
            patterns in model behavior by aligning model explanations with human annotations.</p>
    </d-title>

    <d-byline></d-byline>

    <d-article>
        <a class="marker" href="#section-1" id="section-1"><span>1</span></a>
        <h2>Introduction</h2>
        <p>In AI applications such as cancer diagnosis, autonomous driving, and facial recognition, it is crucial to
            not only understand model performance, but also the reason behind model decisions. Various prior work has
            demonstrated weaknesses in these models &mdash; even highly accurate ones &mdash; including reliance on
            non-salient
            regions
            <d-cite key="carter2020overinterpretation"></d-cite>
            or on background information only
            <d-cite key="xiao2020noise"></d-cite>
            .
            Explanation methods help identify these
            pitfalls by providing explanations for model predictions, enabling humans to identify the features on which a model decision
            is based. However, these methods provide explanations on the image level making it challenging to understand
            global model behavior or dataset limitations.
        </p>
        <p>In this work, we explore model decisions using saliency methods in conjunction with the ground truth object
            bounding boxes provided in
            many computer vision datasets. We introduce three scoring functions &mdash; explanation
            coverage, ground truth coverage, and IoU &mdash; to sort images based on the overlap between the explanation
            and
            the ground truth object location. By sorting images in this way, we discover insights into when and why the
            model was “right for the right reasons”, “wrong for the wrong reasons”, or perhaps most interestingly “right
            for the wrong reasons”. We show our methodology is applicable to various model architectures, explanatory
            methods, and input datasets by evaluating on two representative tasks:
            ImageNet
            <d-cite key="deng2009imagenet"></d-cite>
            vehicle classification using a pretrained
            ResNet50
            <d-cite key="he2016deep"></d-cite>
            model and melanoma prediction using a
            VGG11 model
            <d-cite key="simonyan2014very"></d-cite>
            and the ISIC lesions dataset
            <d-cite key="gutman2016skin"></d-cite>
            .
            In both tasks, we identify insights into model
            behavior and uncover unexpected features of our datasets.
        </p>

        <a class="marker" href="#section-2" id="section-2"><span>2</span></a>
        <h2>Related Work</h2>
        <p>Image datasets from domains such as object
            detection
            <d-cite key="deng2009imagenet"></d-cite>
            <d-cite key="lin2014microsoft"></d-cite>
            <d-cite key="mottaghi2014role"></d-cite>,
             facial recognition
            <d-cite key="liu2015faceattributes"></d-cite>
            <d-cite key="bansal2017umdfaces"></d-cite>,
             and
            medical classification
            <d-cite key="gutman2016skin"></d-cite>
            <d-cite key="wang2017chestx"></d-cite>
            <d-cite key="yan2018deeplesion"></d-cite>
            often contain ground truth object segmentations. In our method, we utilize these ground truth annotations to
            help humans understand model behavior. We evaluate our model on two image datasets with ground
            truth annotations: ImageNet
            <d-cite key="deng2009imagenet"></d-cite>
            which contains human generated bounding boxes and the ISIC lesions dataset
            <d-cite key="gutman2016skin"></d-cite>
            which
            contains clinician defined object segmenations.
        </p>
        <p>In conjunction with ground truth annotations, our method relies on explanation methods (e.g.,
            vanilla gradients<d-cite key="simonyan2013deep"></d-cite>,
             integrated gradients<d-cite key="sundararajan2017axiomatic"></d-cite>,
             LIME<d-cite key="lime"></d-cite>,
             SmoothGrad<d-cite key="smilkov2017smoothgrad"></d-cite>,
             and SIS<d-cite key="carter2019made"></d-cite>)
             that provide pixel-level explanations for model behavior. While these methods successfully expose explanations for a single
            input, our method enables users to sort and rank explanations over all images as a way to gain insight into
            global model behavior.
            Our methodology is agnostic to the explanation method allowing for flexibility across use cases.
        </p>
        <p></p>
        <p>Aside from explainability methods, a growing number of techniques have been developed to help users interpret
            image models <d-cite key="hohman2018visual"></d-cite>.
             However, these methods often focus on
            understanding patterns learned by intermediate nodes
            <d-cite key="netdissect2017"></d-cite>
            <d-cite key="zeiler2014visualizing"></d-cite>
            <d-cite key="hohman2019s"></d-cite>,
             require additional datasets<d-cite key="netdissect2017"></d-cite>,
            or demand architecture specific algorithms<d-cite key="hohman2019s"></d-cite>.
            On the other hand, our method is agnostic to model architecture, saliency method, and dataset enabling
            users across domains to understand global patterns in model behavior by aligning
            model explanations and ground truth annotations.
        </p>

        <a class="marker" href="#section-3" id="section-3"><span>3</span></a>
        <h2>Method</h2>
        <p>In our method, we leverage the ground truth annotations along with instance-level explanations to compute
            coverage scores for each image. By sorting the images using these scores, we are able to query for instances
            that give us insight into model behavior. Our methodology is agnostic to model architecture, dataset, and
            explanation technique, giving it the flexibility to be used across tasks and domains.</p>
        <p>We compute three coverage metrics to allow a breadth of exploration: explanation coverage, ground
            truth coverage, and IoU. Each score takes as input a set of pixels $$GT$$
            corresponding to the known ground truth region and a set of pixels $$E$$ corresponding to
            the explanation region.</p>
        <d-math block>
            \begin{aligned}
                \text{Explanation Coverage} &= \frac{|GT \cap E|}{|E|} \\\\
                \text{Ground Truth Coverage} &= \frac{|GT \cap E|}{|GT|} \\\\
                \text{IoU} &= \frac{|GT \cap E|}{|GT \cup E|}
            \end{aligned}
        </d-math>
        <p>As shown in Figure 1, a low score under all three metrics indicates that an image’s explanation region and
            ground truth region are disjoint. In Table 1, we show example scenarios using an image classification task.
            When a correctly classified image has a low score, it often indicates the model was relying on background
            information such as a snowmobile helmet to make the prediction of snowmobile or train tracks to make the
            prediction of electric locomotive. When an image has a low score and is incorrectly classified, it can
            indicate the model is focusing on a secondary object in the image or incorrectly relying on background
            context (e.g., using snow to predict arctic fox).</p>
        <d-figure>
            <figure style="margin-top: 10px">
                <img src="./assets/img/explain_01.png" alt="Explaining ground truth pt1"
                    style="margin-top: 20px; margin-bottom: 20px;">
                <img src="./assets/img/explain_02a.png" alt="Explaining ground truth pt2" style="margin-bottom: 20px;">
                <figcaption>Figure 1. A visual representation of all three coverage metrics. Ground truth coverage
                    represents the proportion of the ground truth region covered by the explanation region, explanation
                    coverage represents the proportion of the explanation region covered by the ground truth, and IoU
                    represents how similar the two regions are.
                </figcaption>
            </figure>
        </d-figure>
        <p>Explanation coverage represents the proportion of the explanation region covered by the ground truth region.
            High explanation coverage indicates the entire explanation region lies within the ground truth region,
            meaning the model is relying on a subset of salient features to make its prediction. Filtering for correctly
            classified inputs with high explanation coverage can surface instances where a subset of the object, such as
            the dog’s face, was sufficient to make a correct prediction. Looking at incorrectly classified images with
            high explanation coverage can help us find instances where the model uses an insufficient portion of the
            object to make a prediction (e.g., using a small region of black and white spots to predict dalmatian).</p>
        <p>Ground truth coverage represents the proportion of the ground truth region covered by the explanation region.
            High ground truth coverage indicates that the model is using the entire object to make its decision. In
            Table 1, we see filtering for correctly classified images with high ground truth coverage
            uncovers instances where the model relies on the object and relevant background pixels (e.g., the cab and
            the street), to make a correct prediction. Looking at incorrectly classified instances with high ground truth coverage
            shows examples where the model overrelies on contextual information such as using the keyboard and person’s
            lap to predict laptop.
        </p>
        <p>IoU is the strictest metric. A high IoU score indicates the explanation and ground truth are very similar and
            IoU is maximized when the explanation and ground truth regions are identical. Looking at correctly classified images
            with high IoU scores can help identify instances where the model was right for the exactly the right reasons.
            Incorrectly classified images with high IoU scores can surface examples where the image labels are ambiguous
            such as moped and motor scooter.</p>
        <div class="l-page">
            <div id="result-table"></div>
            <figcaption>Table 1. Real world examples for low and high values of all three coverage metrics. Looking at
                correctly and incorrectly classified images at each of the score extremes enables users to find patterns
                in model behavior.
                </figcaption>
        </div>


        <a class="marker" href="#section-4" id="section-4"><span>4</span></a>
        <h2>Case Study: Image Classification</h2>
        <p>We apply our methodology to an image classification task using a ResNet50<d-cite key="he2016deep"></d-cite>
            model pretrained on ImageNet<d-cite key="deng2009imagenet"></d-cite> and
            released publicly by PyTorch<d-cite key="NEURIPS2019_9015"></d-cite>. Since the pretrained model is publicly available, it is widely used in the
            PyTorch community for out of the box classification as well as finetuning and transfer learning tasks. Due
            to the model's popularity and the community’s reliance on it, it is crucial its decisions are grounded in
            salient features.</p>
        <p>In this case study, we apply the model to an ImageNet<d-cite key="deng2009imagenet"></d-cite> classification
            task on a subset of ImageNet9<d-cite key="xiao2020noise"></d-cite> images
            containing vehicles. Each image in the subset contains a single ground truth bounding box annotation for its
            label. For each image we compute a LIME explanation<d-cite key="lime"></d-cite>. Using the overlap metrics
            between the explanation and
            the ground truth bounding box, our methodology allows us to gain insight into model behavior and the
            characteristics of the underlying dataset. </p>
        <p>We begin our exploration by looking at instances where the model performs well. In particular, we choose to
            look at images labeled as Jeep that are classified correctly. To see if the explanations correspond to the
            ground truth regions, we look at images with high IoU scores. We see the model explanations have high
            agreement with the ground truth regions, suggesting its performance on these images is due to having learned
            salient features of Jeeps.</p>
        <d-figure class="l-body-outset">
            <figure>
                <div id="fig2" class="distill-figure"></div>
                <figcaption>Figure 2. Filtering to correctly classified Jeep images with high IoU scores reveals
                    instances with high agreement between the ground truth object region and the explanation.</figcaption>
            </figure>
        </d-figure>
        <p>Looking at the other end of the score distribution, we filter for correctly classified Jeep images with low
            IOU scores. Many of the images still have explanations focused on salient features of Jeeps such as their
            wheels and distinct body shape; however, we notice an example where the explanation region for the class
            Jeep is focused on a black dog. This may indicate that the model has memorized the existence of the black
            dog in the image, raising the question of whether the pixels of the dog contain adversarial properties that
            could cause the model to predict Jeep for any image edited to contain the dog.</p>
        <d-figure class="l-body-outset">
            <figure>
                <div id="fig3" class="distill-figure"></div>
                <figcaption>Figure 3. Looking at correctly classified Jeep images with low IoU scores confirms the model
                    is latching onto salient features except in the first image where the explanation corresponds to the
                    image of a dog.</figcaption>
            </figure>
        </d-figure>
        <p>Finally, we look at incorrectly classified images with low explanation coverage to determine what causes the
            model to fail. Images with low explanation coverage have disjoint explanation and ground truth regions. Without
            looking at the results, we may hypothesize the model makes the incorrect prediction and has a disjoint
            explanation because it is guessing at random. However, looking at the images we see the main failure mode
            occurs when the model predicts a secondary object in the image. Despite each image only having a single
            label and ground truth annotation, we see a large number of images contain multiple objects. </p>
        <d-figure class="l-body-outset">
            <figure>
                <div id="fig4" class="distill-figure"></div>
                <figcaption>Figure 4. Incorrectly classified images with low explanation coverage identify an unexpected
                dataset feature: images with only one ground truth annotation may contain multiple objects.</figcaption>
            </figure>
        </d-figure>
        <p>Our method gives insight into the pretrained PyTorch model predictions on vehicle images from ImageNet,
            showing that the model uses human interpretable explanations for some classes. Further sorting by our
            metrics allows us to discover the dataset contains images with multiple objects which is unexpected given
            the images contain a single label and ground truth annotation.</p>

        <a class="marker" href="#section-5" id="section-5"><span>5</span></a>
        <h2>Case Study: Melanoma Diagnosis</h2>
        <p>In our second case study, we evaluate our method using a melanoma diagnostic task. This case study represents
            a real-world scenario where AI-based melanoma classification
            models<d-cite key="esteva2017dermatologist"></d-cite><d-cite key="ramezani2014automatic"></d-cite><d-cite key="skinvision"></d-cite>
            exist online for at-home risk assessments. Since this is a high-stakes task with impact to patient health,
            it is critical the models rely on salient features when making a prediction.</p>
        <p>In this case study, we use dermoscopic image data from the ISIC Skin Lesion Analysis Towards Melanoma
            Detection 2016 Challenge<d-cite key="gutman2016skin"></d-cite>. Each image in the dataset is an up close image of a skin lesion labeled as either
            benign or malignant and is annotated with a lesion segmentation created by an expert clinician. We train a
            VGG11<d-cite key="simonyan2014very"></d-cite> pretrained on ImageNet<d-cite key="deng2009imagenet"></d-cite> to learn a
            binary benign/malignant classification from the original images and
            achieve accuracy on par with 2016 challenge winners. From the trained model, we extract LIME
            explanations<d-cite key="lime"></d-cite>.
            We evaluate the model by applying our coverage metrics to the ground truth lesion segmentations and LIME
            explanations.</p>
        <p>We begin our exploration by analyzing correctly classified images with the highest IoU scores. These examples
            show the instances where the lesion segmentation and explanation region are most similar. We see there are a
            number of images for which the explanation is focused on the lesion, suggesting the model has learned a
            relationship between lesion characteristics and malignancy.</p>
        <d-figure class="l-body-outset">
            <figure>
                <div id="fig5" class="distill-figure"></div>
                <figcaption>Figure 5. Filtering to correctly classified malignant tumors with high IoU shows images
                where the explanation corresponds with the ground truth lesion.</figcaption>
            </figure>
        </d-figure>
        <p>Since our model seems to be learning some salient features, we next filter to malignant lesions incorrectly
            classified as benign. Sorting by low ground truth coverage, we see there are instances where our model makes
            incorrect predictions relying only on peripheral skin regions. This is particularly concerning in the case
            of at home risk assessment where a cancerous lesion could be classified as benign due to skin surrounding
            the lesion. </p>
        <d-figure class="l-body-outset">
            <figure>
                <div id="fig6" class="distill-figure"></div>
                <figcaption>Figure 6. Looking at images with low ground truth coverage that have been incorrectly classified as
                benign, shows examples where the explanation is disjoint from the lesion.</figcaption>
            </figure>
        </d-figure>
        <p>Since our model incorrectly classified malignant lesions using non-salient background information, we explore
            if the model can also correctly classify lesions without looking at the lesion. We filter to correctly
            classified benign lesions and look for images with low explanation coverage. We find a number of images where the
            model relies on the existence of in-frame dermatological tools to make a benign prediction. While not
            salient, these dermatological tools only exist in benign images and are sufficient to make a correct
            classification.</p>
        <d-figure class="l-body-outset">
            <figure>
                <div id="fig7" class="distill-figure"></div>
                <figcaption>Figure 7. Searching for correctly classified benign images with low explanation coverage
                uncovers cases where the explanation corresponds to dermatological tools within the image.</figcaption>
            </figure>
        </d-figure>
        <p>Using our methodology reveals insight into melanoma model behavior showing that while the model uses salient
            pixels to make some decisions, it dangerously misclassifies malignant tumors due to peripheral skin regions
            and latches onto spurious dataset features. </p>

        <a class="marker" href="#section-6" id="section-6"><span>6</span></a>
        <h2>Conclusion</h2>
        <p>In this work, we present a methodology that enables humans to understand model behavior using the alignment
            between ground truth object labels and saliency method explanations. Our method is agnostic to model
            architecture, explanation method, and image dataset, allowing it to be used in a range of applications. Using
            real world case studies, we show our method allows users to identify
            where the model is “right for the right reasons”, when the model makes correct predictions using non-salient
            features, and when the dataset contains unexpected features.</p>

        <a class="marker" href="#section-7" id="section-7"><span>7</span></a>
        <h2>Try The <a href="../">Demo</a></h2>

    </d-article>

    <d-appendix>
        <d-bibliography src="./assets/bibliography.bib"></d-bibliography>
    </d-appendix>

    <script src="https://d3js.org/d3.v4.js"></script>
    <script src="https://d3js.org/d3-scale-chromatic.v1.js"></script>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
        integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
        integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
        crossorigin="anonymous"></script>
    <script type="text/javascript" src="vendor.js"></script>
    <script type="text/javascript" src="distillMain.js"></script>
</body>

</html>